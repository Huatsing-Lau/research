{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Classification\n",
    "updated: Sep. 01, 2018\n",
    "\n",
    "Data: https://www.physionet.org/pn4/eegmmidb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Downloads\n",
    "\n",
    "### Warning: Executing these blocks will automatically create directories and download datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import urllib\n",
    "\n",
    "# Modeling & Preprocessing\n",
    "import keras.layers as layers\n",
    "from keras.models import Sequential, model_from_json, Model\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from keras import initializers, optimizers, callbacks, models\n",
    "\n",
    "# Essential Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Get Paths\n",
    "from glob import glob\n",
    "\n",
    "# EEG package\n",
    "from mne import find_events, Epochs, concatenate_raws, pick_types\n",
    "from mne.channels import read_montage\n",
    "from mne.io import read_raw_edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = 'pn4/'\n",
    "MATERIAL = 'eegmmidb/'\n",
    "URL = 'https://www.physionet.org/' + CONTEXT + MATERIAL\n",
    "\n",
    "# Change this directory according to your setting\n",
    "USERDIR = '/Users/Jimmy/data/PhysioNet/'\n",
    "\n",
    "page = requests.get(URL).text\n",
    "FOLDERS = sorted(list(set(re.findall(r'S[0-9]+', page))))\n",
    "\n",
    "URLS = [URL+x+'/' for x in FOLDERS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: Executing this block will create folders\n",
    "for folder in FOLDERS:\n",
    "    pathlib.Path(USERDIR +'/'+ folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: Executing this block will start downloading data\n",
    "for i, folder in enumerate(FOLDERS):\n",
    "    page = requests.get(URLS[i]).text\n",
    "    subs = list(set(re.findall(r'S[0-9]+R[0-9]+', page)))\n",
    "    \n",
    "    print('Working on {}, {:.1%} completed'.format(folder, (i+1)/len(FOLDERS)))\n",
    "    for sub in subs:\n",
    "        urllib.request.urlretrieve(URLS[i]+sub+'.edf', os.path.join(USERDIR, folder, sub+'.edf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Raw Data Import\n",
    "\n",
    "I will use a EEG data handling package named MNE (https://martinos.org/mne/stable/index.html) to import raw data and annotation for events from edf files. This package also provides essential signal analysis features, e.g. band-pass filtering. The raw data were filtered using 1Hz of high-pass filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file paths\n",
    "PATH = '/Users/jimmy/data/PhysioNet/'\n",
    "SUBS = glob(PATH + 'S[0-9]*')\n",
    "FNAMES = sorted([x[-4:] for x in SUBS])\n",
    "\n",
    "# Remove subject #89 with damaged data\n",
    "FNAMES.remove('S089')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(subj_num=FNAMES, epoch_sec=0.0625):\n",
    "    \"\"\" Import each subject`s trials and make a 3D array\n",
    "        Output shape: (Trial*Channel*TimeFrames)\n",
    "        \n",
    "        Some edf+ files recorded at low sampling rate, 128Hz, are excluded. \n",
    "        Majority was sampled at 160Hz.\n",
    "        \n",
    "        epoch_sec: time interval for one segment of mashes\n",
    "        \"\"\"\n",
    "    \n",
    "    # Event codes mean different actions for two groups of runs\n",
    "    run_type_0 = '02'.split(',')\n",
    "    run_type_1 = '04,08,12'.split(',')\n",
    "    run_type_2 = '06,10,14'.split(',')\n",
    "\n",
    "    # To calculated completion rate\n",
    "    count = 0\n",
    "    \n",
    "    # Initiate X, y\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # fixed numbers\n",
    "    nChan = 64 \n",
    "    sfreq = 160\n",
    "    sliding = epoch_sec/2 \n",
    "    \n",
    "    # Sub-function to assign X and X, y\n",
    "    def append_X(n_segments, old_x):\n",
    "        new_x = old_x + [data[:, int(sfreq*sliding*n):int(sfreq*sliding*(n+2))] for n in range(n_segments)\\\n",
    "                     if data[:, int(sfreq*sliding*n):int(sfreq*sliding*(n+2))].shape==(nChan, int(sfreq*epoch_sec))]\n",
    "        return new_x\n",
    "    \n",
    "    def append_X_Y(run_type, event, old_x, old_y):\n",
    "        # Number of sliding windows\n",
    "        n_segments = int(event[1]/epoch_sec)*2-1\n",
    "        \n",
    "        # Instantiate new_x, new_y\n",
    "        new_y = old_y\n",
    "        new_x = old_x\n",
    "        \n",
    "        # y assignment\n",
    "        if run_type == 1:\n",
    "            if event[2] == 'T1':\n",
    "                new_y = old_y + [1]*n_segments\n",
    "                new_x = append_X(n_segments, old_x)\n",
    "\n",
    "            elif event[2] == 'T2':\n",
    "                new_y = old_y + [2]*n_segments\n",
    "                new_x = append_X(n_segments, old_x)\n",
    "        \n",
    "        if run_type == 2:\n",
    "            if event[2] == 'T1':\n",
    "                new_y = old_y + [3]*n_segments\n",
    "                new_x = append_X(n_segments, old_x)\n",
    "            \n",
    "            elif event[2] == 'T2':\n",
    "                new_y = old_y + [4]*n_segments\n",
    "                new_x = append_X(n_segments, old_x)\n",
    "        \n",
    "        return new_x, new_y\n",
    "    \n",
    "    # Iterate over subj_num: S001, S002, S003...\n",
    "    for subj in subj_num:\n",
    "        # Return completion rate\n",
    "        count+=1\n",
    "        print('working on {}, {:.1%} completed'.format(subj, count/len(subj_num)))\n",
    "\n",
    "        # Get file names\n",
    "        fnames = glob(os.path.join(PATH, subj, subj+'R*.edf'))\n",
    "        fnames = [name for name in fnames if name[-6:-4] in run_type_0+run_type_1+run_type_2]\n",
    "        \n",
    "        for i, fname in enumerate(fnames):\n",
    "            \n",
    "            # Import data into MNE raw object\n",
    "            raw = read_raw_edf(fname, preload=True, verbose=False)\n",
    "            picks = pick_types(raw.info, eeg=True)\n",
    "            \n",
    "            if raw.info['sfreq'] != 160:\n",
    "                print(f'{subj} is sampled at 128Hz so will be excluded.')\n",
    "                break\n",
    "            \n",
    "            # High-pass filtering\n",
    "            raw.filter(l_freq=1, h_freq=None, picks=picks)\n",
    "            \n",
    "            # Get annotation\n",
    "            events = raw.find_edf_events()\n",
    "            \n",
    "            # Get data\n",
    "            data = raw.get_data(picks=picks)\n",
    "            \n",
    "            # Number of this run\n",
    "            which_run = fname[-6:-4]\n",
    "            \n",
    "            \"\"\" Assignment Starts \"\"\" \n",
    "            # run 1 - baseline (eye closed)\n",
    "            if which_run in run_type_0:\n",
    "\n",
    "                # Number of sliding windows\n",
    "                n_segments = int((raw.n_times/(epoch_sec*sfreq))*2-1)\n",
    "                \n",
    "                # Append 0`s based on number of windows\n",
    "                y.extend([0]*n_segments)\n",
    "                X = append_X(n_segments, X)\n",
    "                    \n",
    "            # run 4,8,12 - imagine opening and closing left or right fist    \n",
    "            elif which_run in run_type_1:\n",
    "                \n",
    "                for i, event in enumerate(events):\n",
    "                    X, y = append_X_Y(run_type=1, event=event, old_x=X, old_y=y)\n",
    "                        \n",
    "            # run 6,10,14 - imagine opening and closing both fists or both feet\n",
    "            elif which_run in run_type_2:\n",
    "                   \n",
    "                for i, event in enumerate(events):         \n",
    "                    X, y = append_X_Y(run_type=2, event=event, old_x=X, old_y=y)\n",
    "                        \n",
    "    X = np.stack(X)\n",
    "    y = np.array(y).reshape((-1,1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "This code is to test MNE raw object\n",
    "\n",
    "subj = FNAMES[0]\n",
    "fnames = glob(os.path.join(PATH, subj, subj+'R*'+'.edf'))\n",
    "raw = read_raw_edf(fnames[5], preload=True, verbose=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on S001, 0.9% completed\n",
      "working on S002, 1.9% completed\n",
      "working on S003, 2.8% completed\n",
      "working on S004, 3.7% completed\n",
      "working on S005, 4.6% completed\n",
      "working on S006, 5.6% completed\n",
      "working on S007, 6.5% completed\n",
      "working on S008, 7.4% completed\n",
      "working on S009, 8.3% completed\n",
      "working on S010, 9.3% completed\n",
      "working on S011, 10.2% completed\n",
      "working on S012, 11.1% completed\n",
      "working on S013, 12.0% completed\n",
      "working on S014, 13.0% completed\n",
      "working on S015, 13.9% completed\n",
      "working on S016, 14.8% completed\n",
      "working on S017, 15.7% completed\n",
      "working on S018, 16.7% completed\n",
      "working on S019, 17.6% completed\n",
      "working on S020, 18.5% completed\n",
      "working on S021, 19.4% completed\n",
      "working on S022, 20.4% completed\n",
      "working on S023, 21.3% completed\n",
      "working on S024, 22.2% completed\n",
      "working on S025, 23.1% completed\n",
      "working on S026, 24.1% completed\n",
      "working on S027, 25.0% completed\n",
      "working on S028, 25.9% completed\n",
      "working on S029, 26.9% completed\n",
      "working on S030, 27.8% completed\n",
      "working on S031, 28.7% completed\n",
      "working on S032, 29.6% completed\n",
      "working on S033, 30.6% completed\n",
      "working on S034, 31.5% completed\n",
      "working on S035, 32.4% completed\n",
      "working on S036, 33.3% completed\n",
      "working on S037, 34.3% completed\n",
      "working on S038, 35.2% completed\n",
      "working on S039, 36.1% completed\n",
      "working on S040, 37.0% completed\n",
      "working on S041, 38.0% completed\n",
      "working on S042, 38.9% completed\n",
      "working on S043, 39.8% completed\n",
      "working on S044, 40.7% completed\n",
      "working on S045, 41.7% completed\n",
      "working on S046, 42.6% completed\n",
      "working on S047, 43.5% completed\n",
      "working on S048, 44.4% completed\n",
      "working on S049, 45.4% completed\n",
      "working on S050, 46.3% completed\n",
      "working on S051, 47.2% completed\n",
      "working on S052, 48.1% completed\n",
      "working on S053, 49.1% completed\n",
      "working on S054, 50.0% completed\n",
      "working on S055, 50.9% completed\n",
      "working on S056, 51.9% completed\n",
      "working on S057, 52.8% completed\n",
      "working on S058, 53.7% completed\n",
      "working on S059, 54.6% completed\n",
      "working on S060, 55.6% completed\n",
      "working on S061, 56.5% completed\n",
      "working on S062, 57.4% completed\n",
      "working on S063, 58.3% completed\n",
      "working on S064, 59.3% completed\n",
      "working on S065, 60.2% completed\n",
      "working on S066, 61.1% completed\n",
      "working on S067, 62.0% completed\n",
      "working on S068, 63.0% completed\n",
      "working on S069, 63.9% completed\n",
      "working on S070, 64.8% completed\n",
      "working on S071, 65.7% completed\n",
      "working on S072, 66.7% completed\n",
      "working on S073, 67.6% completed\n",
      "working on S074, 68.5% completed\n",
      "working on S075, 69.4% completed\n",
      "working on S076, 70.4% completed\n",
      "working on S077, 71.3% completed\n",
      "working on S078, 72.2% completed\n",
      "working on S079, 73.1% completed\n",
      "working on S080, 74.1% completed\n",
      "working on S081, 75.0% completed\n",
      "working on S082, 75.9% completed\n",
      "working on S083, 76.9% completed\n",
      "working on S084, 77.8% completed\n",
      "working on S085, 78.7% completed\n",
      "working on S086, 79.6% completed\n",
      "working on S087, 80.6% completed\n",
      "working on S088, 81.5% completed\n",
      "EDF+ with overlapping events are not fully supported\n",
      "EDF+ with overlapping events are not fully supported\n",
      "S088 is sampled at 128Hz so will be excluded.\n",
      "working on S090, 82.4% completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-55172cf90093>:75: RuntimeWarning: EDF+ with overlapping events are not fully supported\n",
      "  raw = read_raw_edf(fname, preload=True, verbose=False)\n",
      "<ipython-input-7-55172cf90093>:75: RuntimeWarning: EDF+ with overlapping events are not fully supported\n",
      "  raw = read_raw_edf(fname, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on S091, 83.3% completed\n",
      "working on S092, 84.3% completed\n",
      "EDF+ with overlapping events are not fully supported\n",
      "EDF+ with overlapping events are not fully supported\n",
      "S092 is sampled at 128Hz so will be excluded.\n",
      "working on S093, 85.2% completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-55172cf90093>:75: RuntimeWarning: EDF+ with overlapping events are not fully supported\n",
      "  raw = read_raw_edf(fname, preload=True, verbose=False)\n",
      "<ipython-input-7-55172cf90093>:75: RuntimeWarning: EDF+ with overlapping events are not fully supported\n",
      "  raw = read_raw_edf(fname, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on S094, 86.1% completed\n",
      "working on S095, 87.0% completed\n",
      "working on S096, 88.0% completed\n",
      "working on S097, 88.9% completed\n",
      "working on S098, 89.8% completed\n",
      "working on S099, 90.7% completed\n",
      "working on S100, 91.7% completed\n",
      "EDF+ with overlapping events are not fully supported\n",
      "EDF+ with overlapping events are not fully supported\n",
      "S100 is sampled at 128Hz so will be excluded.\n",
      "working on S101, 92.6% completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-55172cf90093>:75: RuntimeWarning: EDF+ with overlapping events are not fully supported\n",
      "  raw = read_raw_edf(fname, preload=True, verbose=False)\n",
      "<ipython-input-7-55172cf90093>:75: RuntimeWarning: EDF+ with overlapping events are not fully supported\n",
      "  raw = read_raw_edf(fname, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on S102, 93.5% completed\n",
      "working on S103, 94.4% completed\n",
      "working on S104, 95.4% completed\n",
      "working on S105, 96.3% completed\n",
      "working on S106, 97.2% completed\n",
      "working on S107, 98.1% completed\n",
      "working on S108, 99.1% completed\n",
      "working on S109, 100.0% completed\n"
     ]
    }
   ],
   "source": [
    "X,y = get_data(FNAMES, epoch_sec=0.0625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1425410, 64, 10)\n",
      "(1425410, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "The original goal of applying neural networks is to exclude hand-crafted algorithms & preprocessing as much as possible. I did not use any proprecessing techniques further than standardization to build an end-to-end classifer from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y backup\n",
    "ori_y = y\n",
    "\n",
    "# y encoding\n",
    "oh = OneHotEncoder()\n",
    "y = oh.fit_transform(ori_y).toarray()\n",
    "\n",
    "# Shuffle trials\n",
    "np.random.seed(43)\n",
    "trials = X.shape[0]\n",
    "shuffle_indices = np.random.permutation(trials)\n",
    "X = X[shuffle_indices]\n",
    "y = y[shuffle_indices]\n",
    "\n",
    "# Test set seperation\n",
    "test_ratio = 0.2\n",
    "train_size = int(trials*(1-test_ratio))\n",
    "X_train, X_test, y_train, y_test = X[:train_size,:,:], X[train_size:,:,:],\\\n",
    "                                    y[:train_size,:], y[train_size:,:]\n",
    "    \n",
    "# Z-score Normalization\n",
    "def scale_data(X):\n",
    "    shape = X.shape\n",
    "    scaler = StandardScaler()\n",
    "    scaled_X = np.zeros((shape[0], shape[1], shape[2]))\n",
    "    for i in range(shape[0]):\n",
    "        for z in range(shape[2]):\n",
    "            scaled_X[i, :, z] = np.squeeze(scaler.fit_transform(X[i, :, z].reshape(-1, 1)))\n",
    "        if i%int(shape[0]/10) == 0:\n",
    "            print('{:.2%} done'.format((i+1)/shape[0]))   \n",
    "    return scaled_X\n",
    "            \n",
    "X_train, X_test  = scale_data(X_train), scale_data(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the EEG recording instrument has 3D locations over the subjects\\` scalp, it is essential for the model to learn from the spatial pattern as well as the temporal pattern. I transformed the data into 2D meshes that represents the locations of the electrodes so that stacked convolutional neural networks can grasp the spatial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Make 2D meshes\n",
    "# Import one raw EEG data to get electrode locations\n",
    "subj = FNAMES[0]\n",
    "fnames = glob(os.path.join(PATH, subj, subj+'R*'+'.edf'))\n",
    "raw = read_raw_edf(fnames[3], preload=True, verbose=False)\n",
    "ch_names = raw.info['ch_names'][:-1]\n",
    "\n",
    "# 'ch_index' is a dictionary - keys: electrodes, vals: column index of electrodes\n",
    "ch_index = {re.findall(\"\\w+[0-9]?\", i)[0]:ch_names.index(i) for i in ch_names}; ch_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mesh(X, ch_index=ch_index):\n",
    "    \n",
    "    mesh = np.zeros((X.shape[0], X.shape[2], 10, 11))\n",
    "    X = np.swapaxes(X, 1, 2)\n",
    "    \n",
    "    # 1st line\n",
    "    mesh[:, :, 0, 4:7] = X[:,:,21:24]; print('1st finished')\n",
    "    \n",
    "    # 2nd line\n",
    "    mesh[:, :, 1, 3:8] = X[:,:,24:29]; print('2nd finished')\n",
    "    \n",
    "    # 3rd line\n",
    "    mesh[:, :, 2, 1:10] = X[:,:,29:38]; print('3rd finished')\n",
    "    \n",
    "    # 4th line\n",
    "    mesh[:, :, 3, 1:10] = np.concatenate((X[:,:,ch_index['Ft7']].reshape(-1, X.shape[1], 1),\\\n",
    "                                          X[:,:,0:7], X[:,:,ch_index['Ft8']].reshape(-1, X.shape[1], 1)), axis=2)\n",
    "    print('4th finished')\n",
    "    \n",
    "    # 5th line\n",
    "    mesh[:, :, 4, 0:11] = np.concatenate((X[:,:,(ch_index['T9'],ch_index['T7'])],\\\n",
    "                                        X[:,:,7:14], X[:,:,(ch_index['T8'],ch_index['T10'])]), axis=2)\n",
    "    print('5th finished')\n",
    "    \n",
    "    # 6th line\n",
    "    mesh[:, :, 5, 1:10] = np.concatenate((X[:,:,ch_index['Tp7']].reshape(-1, X.shape[1], 1),\\\n",
    "                                        X[:,:,14:21], X[:,:,ch_index['Tp8']].reshape(-1, X.shape[1], 1)), axis=2)\n",
    "    print('6th finished')\n",
    "               \n",
    "    # 7th line\n",
    "    mesh[:, :, 6, 1:10] = X[:,:,46:55]; print('7th finished')\n",
    "    \n",
    "    # 8th line\n",
    "    mesh[:, :, 7, 3:8] = X[:,:,55:60]; print('8th finished')\n",
    "    \n",
    "    # 9th line\n",
    "    mesh[:, :, 8, 4:7] = X[:,:,60:63]; print('9th finished')\n",
    "    \n",
    "    # 10th line\n",
    "    mesh[:, :, 9, 5] = X[:,:,63]; print('10th finished')\n",
    "    \n",
    "    return mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make meshes - Dimension: (Sample * Channel * Width * Height)\n",
    "X_train, X_test = convert_mesh(X_train), convert_mesh(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the shape of the mesh\n",
    "np.set_printoptions(precision=2, linewidth=100)\n",
    "X_train[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling - Time-Distributed CNN + RNN\n",
    "\n",
    "Training Plan:\n",
    "\n",
    "+ 4 GPU units (Nvidia Tesla P100) were used to train this neural network.\n",
    "+ Instead of training the whole model at once, I trained the first block (CNN) first. Then using the trained parameters as initial values, I trained the next blocks step-by-step. This approach can greatly reduce the time required for training and help avoiding falling into local minimums.\n",
    "+ The first blocks (CNN) can be applied for other EEG classification models as a pre-trained base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make another dimension, 1, to apply CNN for each time frame.\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Complicated Model - the same as Zhang`s\n",
    "input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3], 1)\n",
    "lecun = initializers.lecun_normal(seed=42)\n",
    "\n",
    "# Input layer\n",
    "inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "# Convolutional layers block\n",
    "x = layers.TimeDistributed(layers.Conv2D(32, (3,3), padding='same', \n",
    "                     data_format='channels_last', kernel_initializer=lecun), name='CNN1')(inputs)\n",
    "x = layers.BatchNormalization(name='batch1')(x)\n",
    "x = layers.Activation('elu', name='act1')(x)\n",
    "x = layers.TimeDistributed(layers.Conv2D(64, (3,3), padding='same', \n",
    "                    data_format='channels_last', kernel_initializer=lecun), name='CNN2')(x)\n",
    "x = layers.BatchNormalization(name='batch2')(x)\n",
    "x = layers.Activation('elu', name='act2')(x)\n",
    "x = layers.TimeDistributed(layers.Conv2D(128, (3,3), padding='same', \n",
    "                    data_format='channels_last', kernel_initializer=lecun), name='CNN3')(x)\n",
    "x = layers.BatchNormalization(name='batch3')(x)\n",
    "x = layers.Activation('elu', name='act3')(x)\n",
    "x = layers.Flatten(name='flatten')(x)\n",
    "\n",
    "# Fully connected layer block\n",
    "y = layers.Dense(1024, kernel_initializer=lecun, name='FC')(x)\n",
    "y = layers.Dropout(0.5)(y, name='dropout1')\n",
    "y = layers.BatchNormalization(name='batch4')(y)\n",
    "y = layers.Activation(activation='elu')(y)\n",
    "\n",
    "# Recurrent layers block\n",
    "z = layers.LSTM(64, kernel_initializer=lecun, return_sequences=True, name='LSTM1')(y)\n",
    "z = layers.LSTM(64, kernel_initializer=lecun, name='LSTM2')(z)\n",
    "\n",
    "# Fully connected layer block\n",
    "FC2 = layers.Dense(128, kernel_initializer=lecun, name='FC2')(RNN2)\n",
    "\n",
    "# Output layer\n",
    "outputs = layers.Dense(5, activation='softmax')(x)\n",
    "\n",
    "# Model compile\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model to transfer pre-trained parameters\n",
    "trans_model = model.load('CNN_3blocks.h5')\n",
    "\n",
    "# Transfer learning - parameter copy & paste\n",
    "which_layer = 'CNN1,CNN2,CNN3,batch1,batch2,batch3'.split(',')\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "trans_layer_names = [layer.name for layer in tran_model.layers]\n",
    "\n",
    "for layer in which_layer:\n",
    "    ind = layer_names.index(layer)\n",
    "    trans_ind = trans_layer_names.index(layer)\n",
    "    model.layers[ind].set_weights(trans_model.layers[trans_ind].get_weights())\n",
    "    \n",
    "for layer in model.layers[:9]: # Freeze the first 9 layers(CNN block)\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn on multi-GPU mode\n",
    "model = multi_gpu_model(model, gpus=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 162732 samples, validate on 40683 samples\n",
      "Epoch 1/50\n",
      "162732/162732 [==============================] - 2283s 14ms/step - loss: 0.9701 - acc: 0.4798 - sd_pred: 0.2174 - val_loss: 0.8359 - val_acc: 0.5198 - val_sd_pred: 0.2467\n",
      "Epoch 2/50\n",
      "162732/162732 [==============================] - 2238s 14ms/step - loss: 0.7845 - acc: 0.5277 - sd_pred: 0.2483 - val_loss: 0.7128 - val_acc: 0.5428 - val_sd_pred: 0.2577\n",
      "Epoch 3/50\n",
      "162732/162732 [==============================] - 2234s 14ms/step - loss: 0.7251 - acc: 0.5428 - sd_pred: 0.2564 - val_loss: 0.7006 - val_acc: 0.5492 - val_sd_pred: 0.2603\n",
      "Epoch 4/50\n",
      "162732/162732 [==============================] - 2227s 14ms/step - loss: 0.7021 - acc: 0.5500 - sd_pred: 0.2594 - val_loss: 0.6869 - val_acc: 0.5562 - val_sd_pred: 0.2612\n",
      "Epoch 5/50\n",
      "162688/162732 [============================>.] - ETA: 0s - loss: 0.7076 - acc: 0.5477 - sd_pred: 0.2586"
     ]
    }
   ],
   "source": [
    "callbacks_list = [callbacks.ModelCheckpoint('model.h5', save_best_only=True, monitor='val_loss'),\n",
    "                 callbacks.EarlyStopping(monitor='acc', patience=3),\n",
    "                 callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5),\n",
    "                 callbacks.TensorBoard(log_dir='./my_log_dir/', histogram=1)]\n",
    "\n",
    "# Start training\n",
    "model.compile(loss='categorical_crossentropy', optimizer=ptimizers.adam(lr=0.001), metrics=['acc'])\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=5000, validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
