{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Classification\n",
    "updated: Aug. 24, 2018\n",
    "\n",
    "Data: https://www.physionet.org/pn4/eegmmidb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Downloads\n",
    "\n",
    "### Warning: Executing these blocks will automatically create directories and download datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import urllib\n",
    "\n",
    "import keras.layers\n",
    "from keras.models import Sequential, model_from_json\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from mne import find_events, Epochs, concatenate_raws, pick_types\n",
    "from mne.channels import read_montage\n",
    "from mne.io import read_raw_edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = 'pn4/'\n",
    "MATERIAL = 'eegmmidb/'\n",
    "URL = 'https://www.physionet.org/' + CONTEXT + MATERIAL\n",
    "\n",
    "USERDIR = '/Users/Jimmy/data/PhysioNet/'\n",
    "\n",
    "page = requests.get(URL).text\n",
    "FOLDERS = sorted(list(set(re.findall(r'S[0-9]+', page))))\n",
    "\n",
    "URLS = [URL+x+'/' for x in FOLDERS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: Executing this block will create folders\n",
    "for folder in FOLDERS:\n",
    "    pathlib.Path(USERDIR +'/'+ folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FOLDERS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0ee21507e1d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFOLDERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURLS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msubs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'S[0-9]+R[0-9]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Working on {}, {:.1%} completed'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFOLDERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FOLDERS' is not defined"
     ]
    }
   ],
   "source": [
    "# Warning: Executing this block will start downloading data\n",
    "for i, folder in enumerate(FOLDERS):\n",
    "    page = requests.get(URLS[i]).text\n",
    "    subs = list(set(re.findall(r'S[0-9]+R[0-9]+', page)))\n",
    "    \n",
    "    print('Working on {}, {:.1%} completed'.format(folder, (i+1)/len(FOLDERS)))\n",
    "    for sub in subs:\n",
    "        urllib.request.urlretrieve(URLS[i]+sub+'.edf', os.path.join(USERDIR, folder, sub+'.edf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Raw Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file paths\n",
    "PATH = '/Users/jimmy/data/PhysioNet/'\n",
    "SUBS = glob(PATH + 'S[0-9]*')\n",
    "FNAMES = sorted([x[-4:] for x in SUBS])\n",
    "\n",
    "# Remove subject #89 with damaged data\n",
    "FNAMES.remove('S089')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event codes mean different actions for two groups of runs\n",
    "run_type_0 = '02'.split(',')\n",
    "run_type_1 = '04,08,12'.split(',')\n",
    "run_type_2 = '06,10,14'.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(subj_num=FNAMES, epoch_sec=0.0625):\n",
    "    \"\"\" Import each subject`s trials and make a 3D array\n",
    "        Output shape: (Trial*Channel*TimeFrames)\n",
    "        \n",
    "        Some edf+ files recorded at low sampling rate, 128Hz, are excluded. \n",
    "        Majority was sampled at 160Hz.\n",
    "        \n",
    "        epoch_sec: time interval for one segment of mashes\n",
    "        sliding: distance of sliding window moving each time \"\"\"\n",
    "    \n",
    "    # To calculated completion rate\n",
    "    count = 0\n",
    "    \n",
    "    # Initiate X, y\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # fixed numbers\n",
    "    nChan = 64 \n",
    "    sfreq = 160\n",
    "    sliding = epoch_sec/2 \n",
    "    \n",
    "    # Sub-function to assign X and X, y\n",
    "    def append_X(n_segments, old_x):\n",
    "        new_x = old_x + [data[:, int(sfreq*sliding*n):int(sfreq*sliding*(n+2))] for n in range(n_segments)\\\n",
    "                     if data[:, int(sfreq*sliding*n):int(sfreq*sliding*(n+2))].shape==(nChan, int(sfreq*epoch_sec))]\n",
    "        return new_x\n",
    "    \n",
    "    def append_X_Y(run_type, event, old_x, old_y):\n",
    "        # Number of sliding windows\n",
    "        n_segments = int(event[1]/epoch_sec)*2-1\n",
    "        \n",
    "        # Instantiate new_x, new_y\n",
    "        new_y = old_y\n",
    "        new_x = old_x\n",
    "        \n",
    "        # y assignment\n",
    "        if run_type == 1:\n",
    "            if event[2] == 'T1':\n",
    "                new_y = old_y + [1]*n_segments\n",
    "                new_x = append_X(n_segments, old_x)\n",
    "\n",
    "            elif event[2] == 'T2':\n",
    "                new_y = old_y + [2]*n_segments\n",
    "                new_x = append_X(n_segments, old_x)\n",
    "        \n",
    "        if run_type == 2:\n",
    "            if event[2] == 'T1':\n",
    "                new_y = old_y + [3]*n_segments\n",
    "                new_x = append_X(n_segments, old_x)\n",
    "            \n",
    "            elif event[2] == 'T2':\n",
    "                new_y = old_y + [4]*n_segments\n",
    "                new_x = append_X(n_segments, old_x)\n",
    "        \n",
    "        return new_x, new_y\n",
    "    \n",
    "    # Iterate over subj_num: S001, S002, S003...\n",
    "    for subj in subj_num:\n",
    "        # Return completion rate\n",
    "        count+=1\n",
    "        print('working on {}, {:.1%} completed'.format(subj, count/len(subj_num)))\n",
    "\n",
    "        # Get file names\n",
    "        fnames = glob(os.path.join(PATH, subj, subj+'R*.edf'))\n",
    "        fnames = [name for name in fnames if name[-6:-4] in run_type_0+run_type_1+run_type_2]\n",
    "        \n",
    "        for i, fname in enumerate(fnames):\n",
    "            \n",
    "            # Import data into MNE raw object\n",
    "            raw = read_raw_edf(fname, preload=True, verbose=False)\n",
    "            picks = pick_types(raw.info, eeg=True)\n",
    "            \n",
    "            if raw.info['sfreq'] != 160:\n",
    "                print(f'{subj} is sampled at 128Hz so will be excluded.')\n",
    "                break\n",
    "            \n",
    "            # High-pass filtering\n",
    "            raw.filter(l_freq=1, h_freq=None, picks=picks)\n",
    "            \n",
    "            # Get annotation\n",
    "            events = raw.find_edf_events()\n",
    "            \n",
    "            # Get data\n",
    "            data = raw.get_data(picks=picks)\n",
    "            \n",
    "            # Number of this run\n",
    "            which_run = fname[-6:-4]\n",
    "            \n",
    "            \"\"\" Assignment Starts \"\"\" \n",
    "            # run 1 - baseline (eye closed)\n",
    "            if which_run in run_type_0:\n",
    "\n",
    "                # Number of sliding windows\n",
    "                n_segments = int((raw.n_times/(epoch_sec*sfreq))*2-1)\n",
    "                \n",
    "                # Append 0`s based on number of windows\n",
    "                y.extend([0]*n_segments)\n",
    "                X = append_X(n_segments, X)\n",
    "                    \n",
    "            # run 4,8,12 - imagine opening and closing left or right fist    \n",
    "            elif which_run in run_type_1:\n",
    "                \n",
    "                for i, event in enumerate(events):\n",
    "                    X, y = append_X_Y(run_type=1, event=event, old_x=X, old_y=y)\n",
    "                        \n",
    "            # run 6,10,14 - imagine opening and closing both fists or both feet\n",
    "            elif which_run in run_type_2:\n",
    "                   \n",
    "                for i, event in enumerate(events):         \n",
    "                    X, y = append_X_Y(run_type=2, event=event, old_x=X, old_y=y)\n",
    "                        \n",
    "    X = np.stack(X)\n",
    "    y = np.array(y).reshape((-1,1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In order to test MNE raw object\n",
    "#subj = FNAMES[0]\n",
    "#fnames = glob(os.path.join(PATH, subj, subj+'R*'+'.edf'))\n",
    "#raw = read_raw_edf(fnames[0], preload=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on S001, 0.9% completed\n",
      "working on S002, 1.9% completed\n",
      "working on S003, 2.8% completed\n",
      "working on S004, 3.7% completed\n",
      "working on S005, 4.6% completed\n",
      "working on S006, 5.6% completed\n",
      "working on S007, 6.5% completed\n",
      "working on S008, 7.4% completed\n",
      "working on S009, 8.3% completed\n",
      "working on S010, 9.3% completed\n",
      "working on S011, 10.2% completed\n",
      "working on S012, 11.1% completed\n",
      "working on S013, 12.0% completed\n",
      "working on S014, 13.0% completed\n",
      "working on S015, 13.9% completed\n",
      "working on S016, 14.8% completed\n",
      "working on S017, 15.7% completed\n",
      "working on S018, 16.7% completed\n",
      "working on S019, 17.6% completed\n",
      "working on S020, 18.5% completed\n",
      "working on S021, 19.4% completed\n",
      "working on S022, 20.4% completed\n",
      "working on S023, 21.3% completed\n",
      "working on S024, 22.2% completed\n",
      "working on S025, 23.1% completed\n",
      "working on S026, 24.1% completed\n",
      "working on S027, 25.0% completed\n",
      "working on S028, 25.9% completed\n",
      "working on S029, 26.9% completed\n",
      "working on S030, 27.8% completed\n",
      "working on S031, 28.7% completed\n",
      "working on S032, 29.6% completed\n",
      "working on S033, 30.6% completed\n",
      "working on S034, 31.5% completed\n",
      "working on S035, 32.4% completed\n",
      "working on S036, 33.3% completed\n",
      "working on S037, 34.3% completed\n",
      "working on S038, 35.2% completed\n",
      "working on S039, 36.1% completed\n",
      "working on S040, 37.0% completed\n",
      "working on S041, 38.0% completed\n",
      "working on S042, 38.9% completed\n",
      "working on S043, 39.8% completed\n",
      "working on S044, 40.7% completed\n",
      "working on S045, 41.7% completed\n",
      "working on S046, 42.6% completed\n",
      "working on S047, 43.5% completed\n",
      "working on S048, 44.4% completed\n",
      "working on S049, 45.4% completed\n",
      "working on S050, 46.3% completed\n",
      "working on S051, 47.2% completed\n",
      "working on S052, 48.1% completed\n",
      "working on S053, 49.1% completed\n",
      "working on S054, 50.0% completed\n",
      "working on S055, 50.9% completed\n",
      "working on S056, 51.9% completed\n",
      "working on S057, 52.8% completed\n",
      "working on S058, 53.7% completed\n",
      "working on S059, 54.6% completed\n",
      "working on S060, 55.6% completed\n",
      "working on S061, 56.5% completed\n",
      "working on S062, 57.4% completed\n",
      "working on S063, 58.3% completed\n"
     ]
    }
   ],
   "source": [
    "X,y = get_data(FNAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y backup\n",
    "ori_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y encoding\n",
    "oh = OneHotEncoder()\n",
    "y = oh.fit_transform(ori_y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle trials\n",
    "np.random.seed(42)\n",
    "trials = X.shape[0]\n",
    "shuffle_indices = np.random.permutation(trials)\n",
    "X = X[shuffle_indices]\n",
    "y = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set seperation\n",
    "test_ratio = 0.2\n",
    "train_size = int(trials*(1-test_ratio))\n",
    "X_train, X_test, y_train, y_test = X[:train_size,:,:], X[train_size:,:,:],\\\n",
    "                                    y[:train_size,:], y[train_size:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use Z-score scaler to reproduce Zhang2018\n",
    "\n",
    "# Min-max scaling for X\n",
    "#train_min = train_X.min(axis=(1,2), keepdims=True)\n",
    "#train_max = train_X.max(axis=(1,2), keepdims=True)\n",
    "#train_X = (train_X - train_min)/(train_max-train_min)\n",
    "\n",
    "#test_min = test_X.min(axis=(1,2), keepdims=True)\n",
    "#test_max = test_X.max(axis=(1,2), keepdims=True)\n",
    "#test_X = (test_X - test_min)/(test_max-test_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1.56% done\n",
      "train 3.12% done\n",
      "train 4.69% done\n",
      "train 6.25% done\n",
      "train 7.81% done\n",
      "train 9.38% done\n",
      "train 10.94% done\n",
      "train 12.50% done\n",
      "train 14.06% done\n",
      "train 15.62% done\n",
      "train 17.19% done\n",
      "train 18.75% done\n",
      "train 20.31% done\n",
      "train 21.88% done\n",
      "train 23.44% done\n",
      "train 25.00% done\n",
      "train 26.56% done\n",
      "train 28.12% done\n",
      "train 29.69% done\n",
      "train 31.25% done\n",
      "train 32.81% done\n",
      "train 34.38% done\n",
      "train 35.94% done\n",
      "train 37.50% done\n",
      "train 39.06% done\n",
      "train 40.62% done\n",
      "train 42.19% done\n",
      "train 43.75% done\n",
      "train 45.31% done\n",
      "train 46.88% done\n",
      "train 48.44% done\n",
      "train 50.00% done\n",
      "train 51.56% done\n",
      "train 53.12% done\n",
      "train 54.69% done\n",
      "train 56.25% done\n",
      "train 57.81% done\n",
      "train 59.38% done\n",
      "train 60.94% done\n",
      "train 62.50% done\n",
      "train 64.06% done\n",
      "train 65.62% done\n",
      "train 67.19% done\n",
      "train 68.75% done\n",
      "train 70.31% done\n",
      "train 71.88% done\n",
      "train 73.44% done\n",
      "train 75.00% done\n",
      "train 76.56% done\n",
      "train 78.12% done\n",
      "train 79.69% done\n",
      "train 81.25% done\n",
      "train 82.81% done\n",
      "train 84.38% done\n",
      "train 85.94% done\n",
      "train 87.50% done\n",
      "train 89.06% done\n",
      "train 90.62% done\n",
      "train 92.19% done\n",
      "train 93.75% done\n",
      "train 95.31% done\n",
      "train 96.88% done\n",
      "train 98.44% done\n",
      "train 100.00% done\n",
      "test 1.56% done\n",
      "test 3.12% done\n",
      "test 4.69% done\n",
      "test 6.25% done\n",
      "test 7.81% done\n",
      "test 9.38% done\n",
      "test 10.94% done\n",
      "test 12.50% done\n",
      "test 14.06% done\n",
      "test 15.62% done\n",
      "test 17.19% done\n",
      "test 18.75% done\n",
      "test 20.31% done\n",
      "test 21.88% done\n",
      "test 23.44% done\n",
      "test 25.00% done\n",
      "test 26.56% done\n",
      "test 28.12% done\n",
      "test 29.69% done\n",
      "test 31.25% done\n",
      "test 32.81% done\n",
      "test 34.38% done\n",
      "test 35.94% done\n",
      "test 37.50% done\n",
      "test 39.06% done\n",
      "test 40.62% done\n",
      "test 42.19% done\n",
      "test 43.75% done\n",
      "test 45.31% done\n",
      "test 46.88% done\n",
      "test 48.44% done\n",
      "test 50.00% done\n",
      "test 51.56% done\n",
      "test 53.12% done\n",
      "test 54.69% done\n",
      "test 56.25% done\n",
      "test 57.81% done\n",
      "test 59.38% done\n",
      "test 60.94% done\n",
      "test 62.50% done\n",
      "test 64.06% done\n",
      "test 65.62% done\n",
      "test 67.19% done\n",
      "test 68.75% done\n",
      "test 70.31% done\n",
      "test 71.88% done\n",
      "test 73.44% done\n",
      "test 75.00% done\n",
      "test 76.56% done\n",
      "test 78.12% done\n",
      "test 79.69% done\n",
      "test 81.25% done\n",
      "test 82.81% done\n",
      "test 84.38% done\n",
      "test 85.94% done\n",
      "test 87.50% done\n",
      "test 89.06% done\n",
      "test 90.62% done\n",
      "test 92.19% done\n",
      "test 93.75% done\n",
      "test 95.31% done\n",
      "test 96.88% done\n",
      "test 98.44% done\n",
      "test 100.00% done\n"
     ]
    }
   ],
   "source": [
    "# Z-score normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalers = {}\n",
    "for i in range(X_train.shape[1]):\n",
    "    scalers[i] = StandardScaler()\n",
    "    X_train[:, i, :] = scalers[i].fit_transform(X_train[:, i, :]) \n",
    "    print('train {:.2%} done'.format((i+1)/X_train.shape[1]))\n",
    "\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test[:, i, :] = scalers[i].transform(X_test[:, i, :]) \n",
    "    print('test {:.2%} done'.format((i+1)/X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fc5': 0,\n",
       " 'Fc3': 1,\n",
       " 'Fc1': 2,\n",
       " 'Fcz': 3,\n",
       " 'Fc2': 4,\n",
       " 'Fc4': 5,\n",
       " 'Fc6': 6,\n",
       " 'C5': 7,\n",
       " 'C3': 8,\n",
       " 'C1': 9,\n",
       " 'Cz': 10,\n",
       " 'C2': 11,\n",
       " 'C4': 12,\n",
       " 'C6': 13,\n",
       " 'Cp5': 14,\n",
       " 'Cp3': 15,\n",
       " 'Cp1': 16,\n",
       " 'Cpz': 17,\n",
       " 'Cp2': 18,\n",
       " 'Cp4': 19,\n",
       " 'Cp6': 20,\n",
       " 'Fp1': 21,\n",
       " 'Fpz': 22,\n",
       " 'Fp2': 23,\n",
       " 'Af7': 24,\n",
       " 'Af3': 25,\n",
       " 'Afz': 26,\n",
       " 'Af4': 27,\n",
       " 'Af8': 28,\n",
       " 'F7': 29,\n",
       " 'F5': 30,\n",
       " 'F3': 31,\n",
       " 'F1': 32,\n",
       " 'Fz': 33,\n",
       " 'F2': 34,\n",
       " 'F4': 35,\n",
       " 'F6': 36,\n",
       " 'F8': 37,\n",
       " 'Ft7': 38,\n",
       " 'Ft8': 39,\n",
       " 'T7': 40,\n",
       " 'T8': 41,\n",
       " 'T9': 42,\n",
       " 'T10': 43,\n",
       " 'Tp7': 44,\n",
       " 'Tp8': 45,\n",
       " 'P7': 46,\n",
       " 'P5': 47,\n",
       " 'P3': 48,\n",
       " 'P1': 49,\n",
       " 'Pz': 50,\n",
       " 'P2': 51,\n",
       " 'P4': 52,\n",
       " 'P6': 53,\n",
       " 'P8': 54,\n",
       " 'Po7': 55,\n",
       " 'Po3': 56,\n",
       " 'Poz': 57,\n",
       " 'Po4': 58,\n",
       " 'Po8': 59,\n",
       " 'O1': 60,\n",
       " 'Oz': 61,\n",
       " 'O2': 62,\n",
       " 'Iz': 63}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make 2D meshes\n",
    "\n",
    "# Import one raw EEG data to get electrode locations\n",
    "subj = FNAMES[0]\n",
    "fnames = glob(os.path.join(PATH, subj, subj+'R*'+'.edf'))\n",
    "raw = read_raw_edf(fnames[3], preload=True, verbose=False)\n",
    "ch_names = raw.info['ch_names'][:-1]\n",
    "\n",
    "# 'ch_index' is a dictionary - keys: electrodes, vals: column index of electrodes\n",
    "ch_index = {re.findall(\"\\w+[0-9]?\", i)[0]:ch_names.index(i) for i in ch_names}; ch_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mesh(X, ch_index=ch_index):\n",
    "    \n",
    "    mesh = np.zeros((X.shape[0], X.shape[2], 10, 11))\n",
    "    \n",
    "    # 1st line\n",
    "    mesh[:, :, 0, 4:7] = np.swapaxes(X[:,21:24,:], 1, 2); print('1st finished')\n",
    "    \n",
    "    # 2nd line\n",
    "    mesh[:, :, 1, 3:8] = np.swapaxes(X[:,24:29,:], 1, 2); print('2nd finished')\n",
    "    \n",
    "    # 3rd line\n",
    "    mesh[:, :, 2, 1:10] = np.swapaxes(X[:,29:38,:], 1, 2); print('3rd finished')\n",
    "    \n",
    "    # 4th line\n",
    "    mesh[:, :, 3, 1:10] = np.c_[X[:,ch_index['Ft7'],:].reshape(X.shape[0],-1,1),\\\n",
    "                        np.swapaxes(X[:,0:7,:], 1, 2), X[:, ch_index['Ft8'], :].reshape(X.shape[0],-1,1)]; print('4th finished')\n",
    "    \n",
    "    # 5th line\n",
    "    mesh[:, :, 4, 0:11] = np.swapaxes(np.concatenate((X[:,(ch_index['T9'],ch_index['T7']),:],\\\n",
    "                        X[:,7:14,:], X[:, (ch_index['T8'],ch_index['T10']), :]), axis=1), 1, 2); print('5th finished')\n",
    "    # 6th line\n",
    "    mesh[:, :, 5, 1:10] = np.c_[X[:,ch_index['Tp7'],:].reshape(X.shape[0],-1,1),\\\n",
    "                        np.swapaxes(X[:,14:21,:], 1, 2), X[:, ch_index['Tp8'], :].reshape(X.shape[0],-1,1)]; print('6th finished')\n",
    "               \n",
    "    # 7th line\n",
    "    mesh[:, :, 6, 1:10] = np.swapaxes(X[:, 46:55, :], 1, 2); print('7th finished')\n",
    "    \n",
    "    # 8th line\n",
    "    mesh[:, :, 7, 3:8] = np.swapaxes(X[:, 55:60, :], 1, 2); print('8th finished')\n",
    "    \n",
    "    # 9th line\n",
    "    mesh[:, :, 8, 4:7] = np.swapaxes(X[:, 60:63, :], 1, 2); print('9th finished')\n",
    "    \n",
    "    # 10th line\n",
    "    mesh[:, :, 9, 5] = X[:, 63, :]; print('10th finished')\n",
    "    \n",
    "    return mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st finished\n",
      "2nd finished\n",
      "3rd finished\n",
      "4th finished\n",
      "5th finished\n",
      "6th finished\n",
      "7th finished\n",
      "8th finished\n",
      "9th finished\n",
      "10th finished\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = convert_mesh(X_train), convert_mesh(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  ,  0.  , -0.07, -0.26, -0.17,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , -0.06, -0.22, -0.25, -0.35, -0.15,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.03, -0.37, -0.24, -0.28, -0.24, -0.13, -0.04,  0.01, -0.18,  0.  ],\n",
       "       [ 0.  , -0.45, -0.43, -0.32, -0.33, -0.29, -0.19, -0.21, -0.33, -0.22,  0.  ],\n",
       "       [-0.58, -0.37, -0.38, -0.31, -0.15, -0.23, -0.15, -0.1 , -0.2 , -0.12,  0.05],\n",
       "       [ 0.  ,  0.05, -0.41, -0.28, -0.15, -0.12, -0.12, -0.09, -0.07, -0.03,  0.  ],\n",
       "       [ 0.  , -0.15, -0.15, -0.12,  0.01,  0.07,  0.03,  0.23,  0.12, -0.05,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.08,  0.06,  0.09,  0.22,  0.11,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  , -0.04,  0.13,  0.14,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.07,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the shape of the mesh\n",
    "np.set_printoptions(precision=2, linewidth=100)\n",
    "X_train[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling - 2D Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-0ca1f4135eb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Cascade Architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
    "\n",
    "# Cascade Architecture\n",
    "model = Sequential()\n",
    "\n",
    "#CNN\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu',\\\n",
    "                        padding='same', input_shape=input_shape, data_format='channels_first'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu',\\\n",
    "                        padding='same', data_format='channels_first'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu',\\\n",
    "                        padding='same', data_format='channels_first'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "#FC\n",
    "model.add(layers.Reshape((-1, )))\n",
    "model.add(layers.Dense(1028, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "#RNN\n",
    "model.add(layers.LSTM(64, dropout=0.2, return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.LSTM(64, dropout=0.2))\n",
    "model.add(layers.Dropout(0.2))\n",
    "#FC\n",
    "model.add(layers.Dense(1028, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "#Sofmax\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"weightBest.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=128, epochs=500, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_X, test_y, verbose=0)*100\n",
    "print(f'The loss:{score[0]}')\n",
    "print(f'The accuracy:{score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_EEG1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model_EEG1.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
