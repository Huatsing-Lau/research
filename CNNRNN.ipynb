{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Classification\n",
    "updated: Aug. 21, 2018\n",
    "\n",
    "Data: https://www.physionet.org/pn4/eegmmidb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Downloads\n",
    "\n",
    "### Warning: Executing these blocks will automatically create directories and download datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = 'pn4/'\n",
    "MATERIAL = 'eegmmidb/'\n",
    "URL = 'https://www.physionet.org/' + CONTEXT + MATERIAL\n",
    "\n",
    "USERDIR = '/Users/Jimmy/data/PhysioNet/'\n",
    "\n",
    "page = requests.get(URL).text\n",
    "FOLDERS = sorted(list(set(re.findall(r'S[0-9]+', page))))\n",
    "\n",
    "URLS = [URL+x+'/' for x in FOLDERS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for folder in FOLDERS:\n",
    "#    pathlib.Path(USERDIR + folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FOLDERS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0ee21507e1d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFOLDERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURLS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msubs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'S[0-9]+R[0-9]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Working on {}, {:.1%} completed'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFOLDERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FOLDERS' is not defined"
     ]
    }
   ],
   "source": [
    "for i, folder in enumerate(FOLDERS):\n",
    "    page = requests.get(URLS[i]).text\n",
    "    subs = list(set(re.findall(r'S[0-9]+R[0-9]+', page)))\n",
    "    \n",
    "    print('Working on {}, {:.1%} completed'.format(folder, (i+1)/len(FOLDERS)))\n",
    "    for sub in subs:\n",
    "        urllib.request.urlretrieve(URLS[i]+sub+'.edf', os.path.join(USERDIR, folder, sub+'.edf'))\n",
    "        urllib.request.urlretrieve(URLS[i]+sub+'.edf.event', os.path.join(USERDIR, folder, sub+'.edf.event'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Raw Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from mne import find_events, Epochs, concatenate_raws, pick_types\n",
    "from mne.channels import read_montage\n",
    "from mne.io import read_raw_edf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file paths\n",
    "PATH = '/Users/jimmy/data/PhysioNet/'\n",
    "SUBS = glob(PATH+ 'S[0-9]*')\n",
    "FNAMES = sorted([x[-4:] for x in SUBS])\n",
    "\n",
    "# Remove subject #89 with damaged data\n",
    "FNAMES.remove('S089')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event codes mean different actions for two groups of runs\n",
    "event_0 = '01,02'.split(',')\n",
    "event_1 = '03,04,07,08,11,12'.split(',')\n",
    "event_2 = '05,06,09,10,13,14'.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(subj_num=FNAMES, include_rest = False):\n",
    "    \"\"\"Import each subject`s trials and make a 3D array\n",
    "        The output shape: (Trial*Channel*TimeFrames)\n",
    "        Some edf+ files recorded at low sampling rate, 128Hz are excluded. \n",
    "        Majority was sampled at 160Hz.\"\"\"\n",
    "    # To calculated the completion rate\n",
    "    count=0\n",
    "    \n",
    "    # Initiate X, y\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    nChan = 64\n",
    "    \n",
    "    # Epoch period\n",
    "    epoch_sec = 0.065 # i.e. 10 rows for one segment\n",
    "    sliding = 0.03125 # i.e. 50% of overlapping between segments\n",
    "    \n",
    "    # Frequencies\n",
    "    sfreq = 160\n",
    "    \n",
    "    for subj in subj_num:\n",
    "        count+=1\n",
    "        print('working on {}, {:.1%} completed'.format(subj, count/len(subj_num)))\n",
    "        \n",
    "        fnames = glob(os.path.join(PATH, subj, subj+'R*'+'.edf'))\n",
    "    \n",
    "\n",
    "        for i, fname in enumerate(fnames):\n",
    "            \n",
    "            # Import data into MNE raw object\n",
    "            raw = read_raw_edf(fname, preload=True, verbose=False)\n",
    "            picks = pick_types(raw.info, eeg=True)\n",
    "                \n",
    "            if raw.info['sfreq'] != 160:\n",
    "                print(f'{subj} is sampled at 128Hz so will be excluded.')\n",
    "                break\n",
    "            \n",
    "            # High-pass filtering\n",
    "            raw.filter(l_freq=1, h_freq=None, picks=picks)\n",
    "            \n",
    "            # Get annotation\n",
    "            events = raw.find_edf_events()\n",
    "            \n",
    "            # Get data\n",
    "            data = raw.get_data(picks=picks)\n",
    "            \n",
    "            # Experiment number 0,1\n",
    "            if fname[-6:-4] in event_0 and include_rest:\n",
    "\n",
    "                # Number of sliding windows\n",
    "                n_segments = int((raw.n_times/(epoch_sec*sfreq))*2-1)\n",
    "                \n",
    "                y.extend([0]*n_segments)\n",
    "                \n",
    "                for n in range(n_segments):\n",
    "                    X.append(data[:, int(sfreq*sliding*n):int(sfreq*sliding*(n+2))])\n",
    "                    \n",
    "                    \n",
    "                    # Check out the shape\n",
    "                    if X[-1].shape != (nChan, int(sfreq*epoch_sec)): \n",
    "                        \n",
    "                        print(F'shape error!: {fname}, {X[-1].shape}') \n",
    "                        X, y = X[:-1], y[:-1]\n",
    "                    \n",
    "            # Experiment number 3,4,7,8,11,12        \n",
    "            if fname[-6:-4] in event_1:\n",
    "                \n",
    "                for i, event in enumerate(events):\n",
    "                    \n",
    "                    if not include_rest and event[2] == 'T0':\n",
    "                        continue\n",
    "                    \n",
    "                    # Number of sliding windows\n",
    "                    n_segments = int((event[1]/(epoch_sec))*2-1)\n",
    "                    \n",
    "                    # y assignment\n",
    "                    if event[2] == 'T0':\n",
    "                        y.extend([0]*n_segments)\n",
    "                    elif event[2] == 'T1':\n",
    "                        y.extend([1]*n_segments)\n",
    "                    elif event[2] == 'T2':\n",
    "                        y.extend([2]*n_segments)\n",
    "                        \n",
    "                    # X assignment    \n",
    "                    for n in range(n_segments):        \n",
    "                        X.append(data[:, int((event[0]+n*sliding)*sfreq):int((event[0]+(n+2)*sliding)*sfreq)])\n",
    "                           \n",
    "                        \n",
    "                        # Check out the shape \n",
    "                        if X[-1].shape != (nChan, int(sfreq*epoch_sec)): \n",
    "                            print(F'shape error!: {fname}, {X[-1].shape}')\n",
    "                            X, y = X[:-1], y[:-1]\n",
    "                        \n",
    "            # Experiment number 5,6,9,10,13,14\n",
    "            elif fname[-6:-4] in event_2:\n",
    "                   \n",
    "                for i, event in enumerate(events):\n",
    "                    \n",
    "                    if not include_rest and event[2] == 'T0':\n",
    "                        continue                  \n",
    "\n",
    "                    # Number of sliding windows\n",
    "                    n_segments = int((event[1]/(epoch_sec))*2-1)\n",
    "                    \n",
    "                    # y assignment\n",
    "                    if event[2] == 'T0':\n",
    "                        y.extend([0]*n_segments)\n",
    "                    elif event[2] == 'T1':\n",
    "                        y.extend([3]*n_segments)\n",
    "                    elif event[2] == 'T2':\n",
    "                        y.extend([4]*n_segments)\n",
    "                        \n",
    "                    for n in range(n_segments):\n",
    "                        X.append(data[:, int((event[0]+n*sliding)*sfreq):int((event[0]+(n+2)*sliding)*sfreq)])\n",
    "                        \n",
    "                        # Check out the shape\n",
    "                        if X[-1].shape != (nChan, int(sfreq*epoch_sec)): \n",
    "                            print(F'shape error!: {fname}, {X[-1].shape}')\n",
    "                            X, y = X[:-1], y[:-1]\n",
    "                        \n",
    "    X = np.stack(X)\n",
    "    y = np.array(y).reshape((-1,1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = FNAMES[0]\n",
    "fnames = glob(os.path.join(PATH, subj, subj+'R*'+'.edf'))\n",
    "raw = read_raw_edf(fnames[5], preload=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on S001, 0.9% completed\n",
      "working on S002, 1.9% completed\n",
      "working on S003, 2.8% completed\n",
      "working on S004, 3.7% completed\n",
      "working on S005, 4.6% completed\n",
      "working on S006, 5.6% completed\n",
      "working on S007, 6.5% completed\n",
      "working on S008, 7.4% completed\n",
      "working on S009, 8.3% completed\n",
      "working on S010, 9.3% completed\n",
      "working on S011, 10.2% completed\n",
      "working on S012, 11.1% completed\n",
      "working on S013, 12.0% completed\n",
      "working on S014, 13.0% completed\n",
      "working on S015, 13.9% completed\n",
      "working on S016, 14.8% completed\n",
      "working on S017, 15.7% completed\n",
      "working on S018, 16.7% completed\n",
      "working on S019, 17.6% completed\n",
      "working on S020, 18.5% completed\n",
      "working on S021, 19.4% completed\n",
      "working on S022, 20.4% completed\n",
      "working on S023, 21.3% completed\n",
      "working on S024, 22.2% completed\n",
      "working on S025, 23.1% completed\n",
      "working on S026, 24.1% completed\n",
      "working on S027, 25.0% completed\n",
      "working on S028, 25.9% completed\n",
      "working on S029, 26.9% completed\n",
      "working on S030, 27.8% completed\n",
      "working on S031, 28.7% completed\n",
      "working on S032, 29.6% completed\n",
      "working on S033, 30.6% completed\n",
      "working on S034, 31.5% completed\n",
      "working on S035, 32.4% completed\n",
      "working on S036, 33.3% completed\n",
      "working on S037, 34.3% completed\n",
      "working on S038, 35.2% completed\n",
      "working on S039, 36.1% completed\n",
      "working on S040, 37.0% completed\n",
      "working on S041, 38.0% completed\n",
      "working on S042, 38.9% completed\n",
      "working on S043, 39.8% completed\n",
      "working on S044, 40.7% completed\n",
      "working on S045, 41.7% completed\n",
      "working on S046, 42.6% completed\n",
      "working on S047, 43.5% completed\n",
      "working on S048, 44.4% completed\n",
      "working on S049, 45.4% completed\n",
      "working on S050, 46.3% completed\n",
      "working on S051, 47.2% completed\n",
      "working on S052, 48.1% completed\n",
      "working on S053, 49.1% completed\n",
      "working on S054, 50.0% completed\n",
      "working on S055, 50.9% completed\n",
      "working on S056, 51.9% completed\n",
      "working on S057, 52.8% completed\n",
      "working on S058, 53.7% completed\n",
      "working on S059, 54.6% completed\n",
      "working on S060, 55.6% completed\n",
      "working on S061, 56.5% completed\n",
      "working on S062, 57.4% completed\n",
      "working on S063, 58.3% completed\n",
      "working on S064, 59.3% completed\n",
      "working on S065, 60.2% completed\n",
      "working on S066, 61.1% completed\n",
      "working on S067, 62.0% completed\n",
      "working on S068, 63.0% completed\n",
      "working on S069, 63.9% completed\n",
      "working on S070, 64.8% completed\n",
      "working on S071, 65.7% completed\n",
      "working on S072, 66.7% completed\n",
      "working on S073, 67.6% completed\n",
      "working on S074, 68.5% completed\n",
      "working on S075, 69.4% completed\n",
      "working on S076, 70.4% completed\n",
      "working on S077, 71.3% completed\n",
      "working on S078, 72.2% completed\n",
      "working on S079, 73.1% completed\n",
      "working on S080, 74.1% completed\n",
      "working on S081, 75.0% completed\n",
      "working on S082, 75.9% completed\n",
      "working on S083, 76.9% completed\n",
      "working on S084, 77.8% completed\n",
      "working on S085, 78.7% completed\n",
      "working on S086, 79.6% completed\n",
      "working on S087, 80.6% completed\n",
      "working on S088, 81.5% completed\n",
      "EDF+ with overlapping events are not fully supported\n",
      "EDF+ with overlapping events are not fully supported\n",
      "S088 is sampled at 128Hz so will be excluded.\n",
      "working on S090, 82.4% completed\n",
      "working on S091, 83.3% completed\n",
      "working on S092, 84.3% completed\n",
      "EDF+ with overlapping events are not fully supported\n",
      "EDF+ with overlapping events are not fully supported\n",
      "S092 is sampled at 128Hz so will be excluded.\n",
      "working on S093, 85.2% completed\n",
      "working on S094, 86.1% completed\n",
      "working on S095, 87.0% completed\n",
      "working on S096, 88.0% completed\n",
      "working on S097, 88.9% completed\n",
      "working on S098, 89.8% completed\n",
      "working on S099, 90.7% completed\n",
      "working on S100, 91.7% completed\n",
      "EDF+ with overlapping events are not fully supported\n",
      "EDF+ with overlapping events are not fully supported\n",
      "S100 is sampled at 128Hz so will be excluded.\n",
      "working on S101, 92.6% completed\n",
      "working on S102, 93.5% completed\n",
      "working on S103, 94.4% completed\n",
      "working on S104, 95.4% completed\n",
      "working on S105, 96.3% completed\n",
      "working on S106, 97.2% completed\n",
      "working on S107, 98.1% completed\n",
      "working on S108, 99.1% completed\n",
      "working on S109, 100.0% completed\n"
     ]
    }
   ],
   "source": [
    "X,y = get_data(FNAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2360480, 64, 10)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2360480, 1)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, Dense, Flatten, MaxPool1D, AveragePooling1D, Dropout, LSTM, embeddings\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y backup\n",
    "ori_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y encoding\n",
    "oh = OneHotEncoder()\n",
    "y = oh.fit_transform(ori_y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle trials\n",
    "np.random.seed(42)\n",
    "trials = X.shape[0]\n",
    "shuffle_indices = np.random.permutation(trials)\n",
    "X = X[shuffle_indices]\n",
    "y = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set seperation\n",
    "test_ratio = 0.2\n",
    "train_size = int(trials*(1-test_ratio))\n",
    "X_train, X_test, y_train, y_test = X[:train_size,:,:], X[train_size:,:,:],\\\n",
    "                                    y[:train_size,:], y[train_size:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use Z-score scaler to reproduce Zhang2018\n",
    "\n",
    "# Min-max scaling for X\n",
    "#train_min = train_X.min(axis=(1,2), keepdims=True)\n",
    "#train_max = train_X.max(axis=(1,2), keepdims=True)\n",
    "#train_X = (train_X - train_min)/(train_max-train_min)\n",
    "\n",
    "#test_min = test_X.min(axis=(1,2), keepdims=True)\n",
    "#test_max = test_X.max(axis=(1,2), keepdims=True)\n",
    "#test_X = (test_X - test_min)/(test_max-test_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1.56% done\n",
      "train 3.12% done\n",
      "train 4.69% done\n",
      "train 6.25% done\n",
      "train 7.81% done\n",
      "train 9.38% done\n",
      "train 10.94% done\n",
      "train 12.50% done\n",
      "train 14.06% done\n",
      "train 15.62% done\n",
      "train 17.19% done\n",
      "train 18.75% done\n",
      "train 20.31% done\n",
      "train 21.88% done\n",
      "train 23.44% done\n",
      "train 25.00% done\n",
      "train 26.56% done\n",
      "train 28.12% done\n",
      "train 29.69% done\n",
      "train 31.25% done\n",
      "train 32.81% done\n",
      "train 34.38% done\n",
      "train 35.94% done\n",
      "train 37.50% done\n",
      "train 39.06% done\n",
      "train 40.62% done\n",
      "train 42.19% done\n",
      "train 43.75% done\n",
      "train 45.31% done\n",
      "train 46.88% done\n",
      "train 48.44% done\n"
     ]
    }
   ],
   "source": [
    "# Z-score normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalers = {}\n",
    "for i in range(X_train.shape[1]):\n",
    "    scalers[i] = StandardScaler()\n",
    "    X_train[:, i, :] = scalers[i].fit_transform(X_train[:, i, :]) \n",
    "    print('train {:.2%} done'.format((i+1)/X_train.shape[1]))\n",
    "\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test[:, i, :] = scalers[i].transform(X_test[:, i, :]) \n",
    "    print('test {:.2%} done'.format((i+1)/X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Make 2D meshes\n",
    "\n",
    "# Import one raw EEG data to get electrode locations\n",
    "subj = FNAMES[0]\n",
    "fnames = glob(os.path.join(PATH, subj, subj+'R*'+'.edf'))\n",
    "raw = read_raw_edf(fnames[3], preload=True, verbose=False)\n",
    "ch_names = raw.info['ch_names'][:-1]\n",
    "\n",
    "# 'ch_index' is a dictionary - keys: electrodes, vals: column index of electrodes\n",
    "ch_index = {re.findall(\"\\w+[0-9]?\", i)[0]:ch_names.index(i) for i in ch_names}; ch_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mesh(X, ch_index=ch_index):\n",
    "    \n",
    "    mesh = np.zeros((X.shape[0], X.shape[2], 10, 11))\n",
    "    \n",
    "    # 1st line\n",
    "    mesh[:, :, 0, 4:7] = np.swapaxes(X[:,21:24,:], 1, 2); print('1st finished')\n",
    "    \n",
    "    # 2nd line\n",
    "    mesh[:, :, 1, 3:8] = np.swapaxes(X[:,24:29,:], 1, 2); print('2nd finished')\n",
    "    \n",
    "    # 3rd line\n",
    "    mesh[:, :, 2, 1:10] = np.swapaxes(X[:,29:38,:], 1, 2); print('3rd finished')\n",
    "    \n",
    "    # 4th line\n",
    "    mesh[:, :, 3, 1:10] = np.c_[X[:,ch_index['Ft7'],:].reshape(X.shape[0],-1,1),\\\n",
    "                        np.swapaxes(X[:,0:7,:], 1, 2), X[:, ch_index['Ft8'], :].reshape(X.shape[0],-1,1)]; print('4st finished')\n",
    "    \n",
    "    # 5th line\n",
    "    mesh[:, :, 4, 0:11] = np.swapaxes(np.concatenate((X[:,(ch_index['T9'],ch_index['T7']),:],\\\n",
    "                        X[:,7:14,:], X[:, (ch_index['T8'],ch_index['T10']), :]), axis=1), 1, 2); print('5st finished')\n",
    "    # 6th line\n",
    "    mesh[:, :, 5, 1:10] = np.c_[X[:,ch_index['Tp7'],:].reshape(X.shape[0],-1,1),\\\n",
    "                        np.swapaxes(X[:,14:21,:], 1, 2), X[:, ch_index['Tp8'], :].reshape(X.shape[0],-1,1)]; print('6st finished')\n",
    "               \n",
    "    # 7th line\n",
    "    mesh[:, :, 6, 1:10] = np.swapaxes(X[:, 46:55, :], 1, 2); print('7st finished')\n",
    "    \n",
    "    # 8th line\n",
    "    mesh[:, :, 7, 3:8] = np.swapaxes(X[:, 55:60, :], 1, 2); print('8st finished')\n",
    "    \n",
    "    # 9th line\n",
    "    mesh[:, :, 8, 4:7] = np.swapaxes(X[:, 60:63, :], 1, 2); print('9st finished')\n",
    "    \n",
    "    # 10th line\n",
    "    mesh[:, :, 9, 5] = X[:, 63, :]; print('10st finished')\n",
    "    \n",
    "    return mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = convert_mesh(X_train), convert_mesh(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  ,  0.  , -0.07, -0.26, -0.17,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , -0.06, -0.22, -0.25, -0.35, -0.15,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.03, -0.37, -0.24, -0.28, -0.24, -0.13, -0.04,  0.01, -0.18,  0.  ],\n",
       "       [ 0.  , -0.45, -0.43, -0.32, -0.33, -0.29, -0.19, -0.21, -0.33, -0.22,  0.  ],\n",
       "       [-0.58, -0.37, -0.38, -0.31, -0.15, -0.23, -0.15, -0.1 , -0.2 , -0.12,  0.05],\n",
       "       [ 0.  ,  0.05, -0.41, -0.28, -0.15, -0.12, -0.12, -0.09, -0.07, -0.03,  0.  ],\n",
       "       [ 0.  , -0.15, -0.15, -0.12,  0.01,  0.07,  0.03,  0.23,  0.12, -0.05,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.08,  0.06,  0.09,  0.22,  0.11,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  , -0.04,  0.13,  0.14,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.07,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the shape of the mesh\n",
    "np.set_printoptions(precision=2, linewidth=100)\n",
    "\n",
    "X_train[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling - 2D Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
    "\n",
    "# Cascade Architecture\n",
    "model = Sequential()\n",
    "\n",
    "#CNN\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu',\\\n",
    "                        padding='same', input_shape=input_shape, data_format='channels_first'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu',\\\n",
    "                        padding='same', data_format='channels_first'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu',\\\n",
    "                        padding='same', data_format='channels_first'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "#FC\n",
    "model.add(layers.Reshape((64, -1)))\n",
    "model.add(layers.Dense(1028, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "#RNN\n",
    "model.add(layers.LSTM(64, dropout=0.2, return_sequences=True))\n",
    "model.add(layers.LSTM(64, dropout=0.2))\n",
    "\n",
    "#FC\n",
    "model.add(layers.Dense(1028, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "#Sofmax\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(train_X, train_y, batch_size=256, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_X, test_y, verbose=0)*100\n",
    "print(f'The accuracy:{score[1]*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_EEG1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model_EEG1.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
