{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Classification\n",
    "updated: Aug. 24, 2018\n",
    "\n",
    "Data: https://www.physionet.org/pn4/eegmmidb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Downloads\n",
    "\n",
    "### Warning: Executing these blocks will automatically create directories and download datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import urllib\n",
    "\n",
    "# Modeling & Preprocessing\n",
    "import keras.layers as layers\n",
    "from keras.models import Sequential, model_from_json\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from keras import initializers, optimizers\n",
    "\n",
    "# Essential Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Get Paths\n",
    "from glob import glob\n",
    "\n",
    "# EEG package\n",
    "from mne import find_events, Epochs, concatenate_raws, pick_types\n",
    "from mne.channels import read_montage\n",
    "from mne.io import read_raw_edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = 'pn4/'\n",
    "MATERIAL = 'eegmmidb/'\n",
    "URL = 'https://www.physionet.org/' + CONTEXT + MATERIAL\n",
    "\n",
    "# Change this directory according to your setting\n",
    "USERDIR = '/Users/Jimmy/data/PhysioNet/'\n",
    "\n",
    "page = requests.get(URL).text\n",
    "FOLDERS = sorted(list(set(re.findall(r'S[0-9]+', page))))\n",
    "\n",
    "URLS = [URL+x+'/' for x in FOLDERS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: Executing this block will create folders\n",
    "for folder in FOLDERS:\n",
    "    pathlib.Path(USERDIR +'/'+ folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FOLDERS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0ee21507e1d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFOLDERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURLS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msubs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'S[0-9]+R[0-9]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Working on {}, {:.1%} completed'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFOLDERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FOLDERS' is not defined"
     ]
    }
   ],
   "source": [
    "# Warning: Executing this block will start downloading data\n",
    "for i, folder in enumerate(FOLDERS):\n",
    "    page = requests.get(URLS[i]).text\n",
    "    subs = list(set(re.findall(r'S[0-9]+R[0-9]+', page)))\n",
    "    \n",
    "    print('Working on {}, {:.1%} completed'.format(folder, (i+1)/len(FOLDERS)))\n",
    "    for sub in subs:\n",
    "        urllib.request.urlretrieve(URLS[i]+sub+'.edf', os.path.join(USERDIR, folder, sub+'.edf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Raw Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file paths\n",
    "PATH = '/Users/jimmy/data/PhysioNet/'\n",
    "SUBS = glob(PATH + 'S[0-9]*')\n",
    "FNAMES = sorted([x[-4:] for x in SUBS])\n",
    "\n",
    "# Remove subject #89 with damaged data\n",
    "FNAMES.remove('S089')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(subj_num=FNAMES, epoch_sec=0.0625):\n",
    "    \"\"\" Import each subject`s trials and make a 3D array\n",
    "        Output shape: (Trial*Channel*TimeFrames)\n",
    "        \n",
    "        Some edf+ files recorded at low sampling rate, 128Hz, are excluded. \n",
    "        Majority was sampled at 160Hz.\n",
    "        \n",
    "        epoch_sec: time interval for one segment of mashes\n",
    "        \"\"\"\n",
    "    \n",
    "    # Event codes mean different actions for two groups of runs\n",
    "    run_type_0 = '02'.split(',')\n",
    "    run_type_1 = '04,08,12'.split(',')\n",
    "    run_type_2 = '06,10,14'.split(',')\n",
    "\n",
    "    # To calculated completion rate\n",
    "    count = 0\n",
    "    \n",
    "    # Initiate X, y\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # fixed numbers\n",
    "    nChan = 64 \n",
    "    sfreq = 160\n",
    "    sliding = epoch_sec/2 \n",
    "    \n",
    "    # Sub-function to assign X and X, y\n",
    "    def append_X(n_segments, old_x):\n",
    "        new_x = old_x + [data[:, int(sfreq*sliding*n):int(sfreq*sliding*(n+2))] for n in range(n_segments)\\\n",
    "                     if data[:, int(sfreq*sliding*n):int(sfreq*sliding*(n+2))].shape==(nChan, int(sfreq*epoch_sec))]\n",
    "        return new_x\n",
    "    \n",
    "    def append_X_Y(run_type, event, old_x, old_y):\n",
    "        # Number of sliding windows\n",
    "        n_segments = int(event[1]/epoch_sec)*2-1\n",
    "        \n",
    "        # Instantiate new_x, new_y\n",
    "        new_y = old_y\n",
    "        new_x = old_x\n",
    "        \n",
    "        # y assignment\n",
    "        if run_type == 1:\n",
    "            if event[2] == 'T1':\n",
    "                new_y = old_y + [1]*n_segments\n",
    "                new_x = append_X(n_segments, old_x)\n",
    "\n",
    "            elif event[2] == 'T2':\n",
    "                new_y = old_y + [2]*n_segments\n",
    "                new_x = append_X(n_segments, old_x)\n",
    "        \n",
    "        if run_type == 2:\n",
    "            if event[2] == 'T1':\n",
    "                new_y = old_y + [3]*n_segments\n",
    "                new_x = append_X(n_segments, old_x)\n",
    "            \n",
    "            elif event[2] == 'T2':\n",
    "                new_y = old_y + [4]*n_segments\n",
    "                new_x = append_X(n_segments, old_x)\n",
    "        \n",
    "        return new_x, new_y\n",
    "    \n",
    "    # Iterate over subj_num: S001, S002, S003...\n",
    "    for subj in subj_num:\n",
    "        # Return completion rate\n",
    "        count+=1\n",
    "        print('working on {}, {:.1%} completed'.format(subj, count/len(subj_num)))\n",
    "\n",
    "        # Get file names\n",
    "        fnames = glob(os.path.join(PATH, subj, subj+'R*.edf'))\n",
    "        fnames = [name for name in fnames if name[-6:-4] in run_type_0+run_type_1+run_type_2]\n",
    "        \n",
    "        for i, fname in enumerate(fnames):\n",
    "            \n",
    "            # Import data into MNE raw object\n",
    "            raw = read_raw_edf(fname, preload=True, verbose=False)\n",
    "            picks = pick_types(raw.info, eeg=True)\n",
    "            \n",
    "            if raw.info['sfreq'] != 160:\n",
    "                print(f'{subj} is sampled at 128Hz so will be excluded.')\n",
    "                break\n",
    "            \n",
    "            # Get annotation\n",
    "            events = raw.find_edf_events()\n",
    "            \n",
    "            # Get data\n",
    "            data = raw.get_data(picks=picks)\n",
    "            \n",
    "            # Number of this run\n",
    "            which_run = fname[-6:-4]\n",
    "            \n",
    "            \"\"\" Assignment Starts \"\"\" \n",
    "            # run 1 - baseline (eye closed)\n",
    "            if which_run in run_type_0:\n",
    "\n",
    "                # Number of sliding windows\n",
    "                n_segments = int((raw.n_times/(epoch_sec*sfreq))*2-1)\n",
    "                \n",
    "                # Append 0`s based on number of windows\n",
    "                y.extend([0]*n_segments)\n",
    "                X = append_X(n_segments, X)\n",
    "                    \n",
    "            # run 4,8,12 - imagine opening and closing left or right fist    \n",
    "            elif which_run in run_type_1:\n",
    "                \n",
    "                for i, event in enumerate(events):\n",
    "                    X, y = append_X_Y(run_type=1, event=event, old_x=X, old_y=y)\n",
    "                        \n",
    "            # run 6,10,14 - imagine opening and closing both fists or both feet\n",
    "            elif which_run in run_type_2:\n",
    "                   \n",
    "                for i, event in enumerate(events):         \n",
    "                    X, y = append_X_Y(run_type=2, event=event, old_x=X, old_y=y)\n",
    "                        \n",
    "    X = np.stack(X)\n",
    "    y = np.array(y).reshape((-1,1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In order to test MNE raw object\n",
    "#subj = FNAMES[0]\n",
    "#fnames = glob(os.path.join(PATH, subj, subj+'R*'+'.edf'))\n",
    "#raw = read_raw_edf(fnames[5], preload=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on S001, 2.0% completed\n",
      "working on S002, 4.0% completed\n",
      "working on S003, 6.0% completed\n",
      "working on S004, 8.0% completed\n",
      "working on S005, 10.0% completed\n",
      "working on S006, 12.0% completed\n",
      "working on S007, 14.0% completed\n",
      "working on S008, 16.0% completed\n",
      "working on S009, 18.0% completed\n",
      "working on S010, 20.0% completed\n",
      "working on S011, 22.0% completed\n",
      "working on S012, 24.0% completed\n",
      "working on S013, 26.0% completed\n",
      "working on S014, 28.0% completed\n",
      "working on S015, 30.0% completed\n",
      "working on S016, 32.0% completed\n",
      "working on S017, 34.0% completed\n",
      "working on S018, 36.0% completed\n",
      "working on S019, 38.0% completed\n",
      "working on S020, 40.0% completed\n",
      "working on S021, 42.0% completed\n",
      "working on S022, 44.0% completed\n",
      "working on S023, 46.0% completed\n",
      "working on S024, 48.0% completed\n",
      "working on S025, 50.0% completed\n",
      "working on S026, 52.0% completed\n",
      "working on S027, 54.0% completed\n",
      "working on S028, 56.0% completed\n",
      "working on S029, 58.0% completed\n",
      "working on S030, 60.0% completed\n",
      "working on S031, 62.0% completed\n",
      "working on S032, 64.0% completed\n",
      "working on S033, 66.0% completed\n",
      "working on S034, 68.0% completed\n",
      "working on S035, 70.0% completed\n",
      "working on S036, 72.0% completed\n",
      "working on S037, 74.0% completed\n",
      "working on S038, 76.0% completed\n",
      "working on S039, 78.0% completed\n",
      "working on S040, 80.0% completed\n",
      "working on S041, 82.0% completed\n",
      "working on S042, 84.0% completed\n",
      "working on S043, 86.0% completed\n",
      "working on S044, 88.0% completed\n",
      "working on S045, 90.0% completed\n",
      "working on S046, 92.0% completed\n",
      "working on S047, 94.0% completed\n",
      "working on S048, 96.0% completed\n",
      "working on S049, 98.0% completed\n",
      "working on S050, 100.0% completed\n"
     ]
    }
   ],
   "source": [
    "X,y = get_data(FNAMES, epoch_sec=0.0625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-23ee3abe9105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y backup\n",
    "ori_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y encoding\n",
    "oh = OneHotEncoder()\n",
    "y = oh.fit_transform(ori_y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle trials\n",
    "np.random.seed(43)\n",
    "trials = X.shape[0]\n",
    "shuffle_indices = np.random.permutation(trials)\n",
    "X = X[shuffle_indices]\n",
    "y = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set seperation\n",
    "test_ratio = 0.2\n",
    "train_size = int(trials*(1-test_ratio))\n",
    "X_train, X_test, y_train, y_test = X[:train_size,:,:], X[train_size:,:,:],\\\n",
    "                                    y[:train_size,:], y[train_size:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00% done\n",
      "10.00% done\n",
      "20.00% done\n",
      "30.00% done\n",
      "40.00% done\n",
      "50.00% done\n",
      "60.00% done\n",
      "70.00% done\n",
      "80.00% done\n",
      "90.00% done\n",
      "100.00% done\n",
      "0.00% done\n",
      "10.00% done\n",
      "20.00% done\n",
      "30.00% done\n",
      "40.00% done\n",
      "50.00% done\n",
      "60.00% done\n",
      "70.00% done\n",
      "80.00% done\n",
      "90.00% done\n",
      "100.00% done\n"
     ]
    }
   ],
   "source": [
    "# Z-score Normalization\n",
    "def scale_data(X):\n",
    "    shape = X.shape\n",
    "    scaler = StandardScaler()\n",
    "    scaled_X = np.zeros((shape[0], shape[1], shape[2]))\n",
    "    for i in range(shape[0]):\n",
    "        for z in range(shape[2]):\n",
    "            scaled_X[i, :, z] = np.squeeze(scaler.fit_transform(X[i, :, z].reshape(-1, 1)))\n",
    "        if i%int(shape[0]/10) == 0:\n",
    "            print('{:.2%} done'.format((i+1)/shape[0]))   \n",
    "    return scaled_X\n",
    "            \n",
    "X_train, X_test  = scale_data(X_train), scale_data(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fc5': 0,\n",
       " 'Fc3': 1,\n",
       " 'Fc1': 2,\n",
       " 'Fcz': 3,\n",
       " 'Fc2': 4,\n",
       " 'Fc4': 5,\n",
       " 'Fc6': 6,\n",
       " 'C5': 7,\n",
       " 'C3': 8,\n",
       " 'C1': 9,\n",
       " 'Cz': 10,\n",
       " 'C2': 11,\n",
       " 'C4': 12,\n",
       " 'C6': 13,\n",
       " 'Cp5': 14,\n",
       " 'Cp3': 15,\n",
       " 'Cp1': 16,\n",
       " 'Cpz': 17,\n",
       " 'Cp2': 18,\n",
       " 'Cp4': 19,\n",
       " 'Cp6': 20,\n",
       " 'Fp1': 21,\n",
       " 'Fpz': 22,\n",
       " 'Fp2': 23,\n",
       " 'Af7': 24,\n",
       " 'Af3': 25,\n",
       " 'Afz': 26,\n",
       " 'Af4': 27,\n",
       " 'Af8': 28,\n",
       " 'F7': 29,\n",
       " 'F5': 30,\n",
       " 'F3': 31,\n",
       " 'F1': 32,\n",
       " 'Fz': 33,\n",
       " 'F2': 34,\n",
       " 'F4': 35,\n",
       " 'F6': 36,\n",
       " 'F8': 37,\n",
       " 'Ft7': 38,\n",
       " 'Ft8': 39,\n",
       " 'T7': 40,\n",
       " 'T8': 41,\n",
       " 'T9': 42,\n",
       " 'T10': 43,\n",
       " 'Tp7': 44,\n",
       " 'Tp8': 45,\n",
       " 'P7': 46,\n",
       " 'P5': 47,\n",
       " 'P3': 48,\n",
       " 'P1': 49,\n",
       " 'Pz': 50,\n",
       " 'P2': 51,\n",
       " 'P4': 52,\n",
       " 'P6': 53,\n",
       " 'P8': 54,\n",
       " 'Po7': 55,\n",
       " 'Po3': 56,\n",
       " 'Poz': 57,\n",
       " 'Po4': 58,\n",
       " 'Po8': 59,\n",
       " 'O1': 60,\n",
       " 'Oz': 61,\n",
       " 'O2': 62,\n",
       " 'Iz': 63}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make 2D meshes\n",
    "# Import one raw EEG data to get electrode locations\n",
    "subj = FNAMES[0]\n",
    "fnames = glob(os.path.join(PATH, subj, subj+'R*'+'.edf'))\n",
    "raw = read_raw_edf(fnames[3], preload=True, verbose=False)\n",
    "ch_names = raw.info['ch_names'][:-1]\n",
    "\n",
    "# 'ch_index' is a dictionary - keys: electrodes, vals: column index of electrodes\n",
    "ch_index = {re.findall(\"\\w+[0-9]?\", i)[0]:ch_names.index(i) for i in ch_names}; ch_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mesh(X, ch_index=ch_index):\n",
    "    \n",
    "    mesh = np.zeros((X.shape[0], X.shape[2], 10, 11))\n",
    "    X = np.swapaxes(X, 1, 2)\n",
    "    \n",
    "    # 1st line\n",
    "    mesh[:, :, 0, 4:7] = X[:,:,21:24]; print('1st finished')\n",
    "    \n",
    "    # 2nd line\n",
    "    mesh[:, :, 1, 3:8] = X[:,:,24:29]; print('2nd finished')\n",
    "    \n",
    "    # 3rd line\n",
    "    mesh[:, :, 2, 1:10] = X[:,:,29:38]; print('3rd finished')\n",
    "    \n",
    "    # 4th line\n",
    "    mesh[:, :, 3, 1:10] = np.concatenate((X[:,:,ch_index['Ft7']].reshape(-1, X.shape[1], 1),\\\n",
    "                                          X[:,:,0:7], X[:,:,ch_index['Ft8']].reshape(-1, X.shape[1], 1)), axis=2)\n",
    "    print('4th finished')\n",
    "    \n",
    "    # 5th line\n",
    "    mesh[:, :, 4, 0:11] = np.concatenate((X[:,:,(ch_index['T9'],ch_index['T7'])],\\\n",
    "                                        X[:,:,7:14], X[:,:,(ch_index['T8'],ch_index['T10'])]), axis=2)\n",
    "    print('5th finished')\n",
    "    \n",
    "    # 6th line\n",
    "    mesh[:, :, 5, 1:10] = np.concatenate((X[:,:,ch_index['Tp7']].reshape(-1, X.shape[1], 1),\\\n",
    "                                        X[:,:,14:21], X[:,:,ch_index['Tp8']].reshape(-1, X.shape[1], 1)), axis=2)\n",
    "    print('6th finished')\n",
    "               \n",
    "    # 7th line\n",
    "    mesh[:, :, 6, 1:10] = X[:,:,46:55]; print('7th finished')\n",
    "    \n",
    "    # 8th line\n",
    "    mesh[:, :, 7, 3:8] = X[:,:,55:60]; print('8th finished')\n",
    "    \n",
    "    # 9th line\n",
    "    mesh[:, :, 8, 4:7] = X[:,:,60:63]; print('9th finished')\n",
    "    \n",
    "    # 10th line\n",
    "    mesh[:, :, 9, 5] = X[:,:,63]; print('10th finished')\n",
    "    \n",
    "    return mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st finished\n",
      "2nd finished\n",
      "3rd finished\n",
      "4th finished\n",
      "5th finished\n",
      "6th finished\n",
      "7th finished\n",
      "8th finished\n",
      "9th finished\n",
      "10th finished\n",
      "1st finished\n",
      "2nd finished\n",
      "3rd finished\n",
      "4th finished\n",
      "5th finished\n",
      "6th finished\n",
      "7th finished\n",
      "8th finished\n",
      "9th finished\n",
      "10th finished\n"
     ]
    }
   ],
   "source": [
    "# Make meshes - Dimension: (Sample * Channel * Width * Height)\n",
    "X_train, X_test = convert_mesh(X_train), convert_mesh(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  ,  0.  , -1.29, -0.13, -1.19,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , -1.64, -0.73, -0.68, -1.39,  0.18,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  1.59,  0.28,  0.23,  0.63,  0.18,  0.33,  0.43,  0.68,  0.53,  0.  ],\n",
       "       [ 0.  ,  1.79,  1.39,  1.49,  1.54,  0.43,  1.24,  0.73,  1.24, -0.13,  0.  ],\n",
       "       [ 0.93,  1.24,  2.4 ,  1.19,  0.33, -0.83,  0.02,  0.13,  0.93,  0.33,  0.63],\n",
       "       [ 0.  ,  1.49,  0.73,  0.53, -0.33, -1.19, -0.94, -0.94, -0.38, -0.33,  0.  ],\n",
       "       [ 0.  ,  0.43, -0.18,  0.23, -0.43, -1.09, -1.9 , -1.8 , -1.19, -1.74,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , -0.53,  0.02, -1.04, -1.34, -0.33,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  , -1.14, -0.08, -1.29,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.28,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the shape of the mesh\n",
    "np.set_printoptions(precision=2, linewidth=100)\n",
    "X_train[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling - Time-Distributed CNN + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make another dimension, 1, to apply CNN for each time frame.\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_2 (TimeDist (None, 10, 128)           920064    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 10, 16)            9280      \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 932,165\n",
      "Trainable params: 932,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Simplified Model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3], 1)\n",
    "\n",
    "#CNN\n",
    "CNN = Sequential()\n",
    "CNN.add(layers.Conv2D(32, (3,3), padding='same', activation='elu', data_format='channels_last',kernel_initializer=lecun))\n",
    "CNN.add(layers.Conv2D(64, (3,3), padding='same', activation='elu', data_format='channels_last',kernel_initializer=lecun))\n",
    "CNN.add(layers.Flatten())\n",
    "CNN.add(layers.Dense(128, activation='elu', kernel_initializer=lecun))\n",
    "CNN.add(layers.Dropout(0.3))\n",
    "\n",
    "#RNN\n",
    "model = Sequential()\n",
    "model.add(layers.TimeDistributed(CNN, input_shape=input_shape))\n",
    "model.add(layers.LSTM(16, return_sequences=True, kernel_initializer=lecun))\n",
    "model.add(layers.LSTM(16,kernel_initializer=lecun))\n",
    "model.add(layers.Dense(32, activation='elu', kernel_initializer=lecun))\n",
    "\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 162732 samples, validate on 40683 samples\n",
      "Epoch 1/50\n",
      "162732/162732 [==============================] - 2283s 14ms/step - loss: 0.9701 - acc: 0.4798 - sd_pred: 0.2174 - val_loss: 0.8359 - val_acc: 0.5198 - val_sd_pred: 0.2467\n",
      "Epoch 2/50\n",
      "162732/162732 [==============================] - 2238s 14ms/step - loss: 0.7845 - acc: 0.5277 - sd_pred: 0.2483 - val_loss: 0.7128 - val_acc: 0.5428 - val_sd_pred: 0.2577\n",
      "Epoch 3/50\n",
      "162732/162732 [==============================] - 2234s 14ms/step - loss: 0.7251 - acc: 0.5428 - sd_pred: 0.2564 - val_loss: 0.7006 - val_acc: 0.5492 - val_sd_pred: 0.2603\n",
      "Epoch 4/50\n",
      "162732/162732 [==============================] - 2227s 14ms/step - loss: 0.7021 - acc: 0.5500 - sd_pred: 0.2594 - val_loss: 0.6869 - val_acc: 0.5562 - val_sd_pred: 0.2612\n",
      "Epoch 5/50\n",
      "162688/162732 [============================>.] - ETA: 0s - loss: 0.7076 - acc: 0.5477 - sd_pred: 0.2586"
     ]
    }
   ],
   "source": [
    "lecun = initializers.lecun_normal(seed=42)\n",
    "adam = optimizers.adam(lr=0.001)\n",
    "\n",
    "def sd_pred(y_true, y_pred):\n",
    "    return K.std(y_pred)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy', sd_pred])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=64, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-0ca1f4135eb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Cascade Architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "## Complicated Model - the same as Zheng`s\n",
    "input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3], 1)\n",
    "\n",
    "#CNN\n",
    "CNN = Sequential()\n",
    "CNN.add(layers.Conv2D(32, (3,3), padding='same', activation='elu', data_format='channels_last',kernel_initializer=lecun))\n",
    "CNN.add(layers.Conv2D(64, (3,3), padding='same', activation='elu', data_format='channels_last',kernel_initializer=lecun))\n",
    "CNN.add(layers.Conv2D(128, (3,3), padding='same', activation='elu', data_format='channels_last',kernel_initializer=lecun))\n",
    "CNN.add(layers.Flatten())\n",
    "CNN.add(layers.Dense(1024, activation='elu', kernel_initializer=lecun))\n",
    "CNN.add(layers.Dropout(0.5))\n",
    "\n",
    "#RNN\n",
    "model = Sequential()\n",
    "model.add(layers.TimeDistributed(CNN, input_shape=input_shape))\n",
    "model.add(layers.LSTM(64, return_sequences=True, kernel_initializer=lecun))\n",
    "model.add(layers.LSTM(64,kernel_initializer=lecun))\n",
    "model.add(layers.Dense(1024, activation='elu', kernel_initializer=lecun))\n",
    "CNN.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecun = initializers.lecun_normal(seed=42)\n",
    "adam = optimizers.adam(lr=0.001)\n",
    "\n",
    "def sd_pred(y_true, y_pred):\n",
    "    return K.std(y_pred)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy', sd_pred])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=64, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_EEG1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "#json_file = open('model_EEG1.json', 'r')\n",
    "#loaded_model_json = json_file.read()\n",
    "#json_file.close()\n",
    "#loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "#loaded_model.load_weights(\"model.h5\")\n",
    "#print(\"Loaded model from disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
